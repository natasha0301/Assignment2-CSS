{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02467 Assignment 2\n",
    "s204085 Cornelius Erichs & s204076 Natasha Hougaard\n",
    "\n",
    "#### GitHub repository\n",
    "Here is our repository on GitHub: https://github.com/natasha0301/Assigment2-CSS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Mixing Patterns and Assortativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each node, compute the fraction of edges that connect to a node that works in the same top field. Find the average value across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5842856084183233\n"
     ]
    }
   ],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "import networkx as nx\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_json_file(filename):\n",
    "    with open(filename) as f:\n",
    "        js_graph = json.load(f)\n",
    "    return json_graph.node_link_graph(js_graph)\n",
    "G = read_json_file(\"data_total.json\")\n",
    "\n",
    "def same_field(G):\n",
    "    same_field_fractions = []\n",
    "    for node in G.nodes():\n",
    "        same_field_neighbors = 0\n",
    "        total_neighbors = 0\n",
    "        \n",
    "        for neighbor in G.neighbors(node):\n",
    "            if G.nodes[neighbor][\"field\"] == G.nodes[node][\"field\"]:\n",
    "                same_field_neighbors += 1\n",
    "            total_neighbors += 1\n",
    "        \n",
    "        if total_neighbors > 0:\n",
    "            same_field_fraction = same_field_neighbors / total_neighbors\n",
    "        else:\n",
    "            same_field_fraction = 0\n",
    "        same_field_fractions.append(same_field_fraction)\n",
    "    return same_field_fractions\n",
    "\n",
    "same_field_fractions = same_field(G)\n",
    "print(np.mean(same_field(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new graph, with the same nodes and edges, but where the association between nodes and field is shuffled. Compute the measure above for this randomized graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1787791531350391\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "shuffled_G = G.copy()\n",
    "\n",
    "fields = [G.nodes[node][\"field\"] for node in G.nodes()]\n",
    "random.shuffle(fields)\n",
    "\n",
    "for i, node in enumerate(shuffled_G.nodes()):\n",
    "    shuffled_G.nodes[node][\"field\"] = fields[i]\n",
    "    \n",
    "print(np.mean(same_field(shuffled_G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the point above 100 times (at least). Plot the distribution of the values obtained and compare it with the value you have found for the real graph. Is the chance to connect to a member of the same field significantly higher than it would be by chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "results = []\n",
    "for i in range(500):\n",
    "    random.shuffle(fields)\n",
    "\n",
    "    for i, node in enumerate(shuffled_G.nodes()):\n",
    "        shuffled_G.nodes[node][\"field\"] = fields[i]\n",
    "    \n",
    "    results.append(np.mean(same_field(shuffled_G)))\n",
    "    \n",
    "plt.hist(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the assortativity coefficient with respect to author's field. How do you interpret the value you obtain? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agricultural And Food Sciences' 'Art' 'Business' 'Chemistry'\n",
      " 'Computer Science' 'Economics' 'Education' 'Engineering'\n",
      " 'Environmental Science' 'Geography' 'Geology' 'History' 'Law'\n",
      " 'Linguistics' 'Materials Science' 'Mathematics' 'Medicine' 'Philosophy'\n",
      " 'Physics' 'Political Science' 'Psychology' 'Sociology' 'nan']\n",
      "0.7106920785154831\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(fields))\n",
    "unique_fields = {'Agricultural And Food Sciences' : 0,\n",
    "            'Art' : 1,\n",
    "            'Business' : 2,\n",
    "            'Chemistry' : 3,\n",
    "            'Computer Science' : 4,\n",
    "            'Economics' : 5,\n",
    "            'Education' : 6,\n",
    "            'Engineering' : 7,\n",
    "            'Environmental Science' : 8,\n",
    "            'Geography' : 9,\n",
    "            'Geology' : 10,\n",
    "            'History' : 11,\n",
    "            'Law' : 12,\n",
    "            'Linguistics': 13,\n",
    "            'Materials Science': 14,\n",
    "            'Mathematics': 15,\n",
    "            'Medicine': 16,\n",
    "            'Philosophy' : 17,\n",
    "            'Physics' : 18,\n",
    "            'Political Science' : 19,\n",
    "            'Psychology' : 20,\n",
    "            'Sociology' : 21,\n",
    "            None:22}\n",
    "\n",
    "matrix = np.zeros((len(unique_fields),len(unique_fields)))\n",
    "\n",
    "values = nx.get_node_attributes(G, \"field\").values()\n",
    "num_values = len(values)\n",
    "\n",
    "for start, end in G.edges(): # Looping over all edges in graph (since its undirected its not really start and stop)\n",
    "    x = G.nodes[start][\"field\"] # Getting the start point of the edge \n",
    "    y = G.nodes[end][\"field\"] # Getting the end point of the edge \n",
    "    if x in unique_fields: \n",
    "        x = unique_fields[x]\n",
    "    else:\n",
    "        x = unique_fields[None] # in case x is nan\n",
    "        \n",
    "    if y in unique_fields:\n",
    "        y = unique_fields[y]\n",
    "    else:\n",
    "        y = unique_fields[None] # in case y is nan\n",
    "    matrix[x, y] += 1 \n",
    "    \n",
    "num_edges = len(G.edges())\n",
    "matrix /= num_edges # averaging the occurence with the total edges\n",
    "\n",
    "trace = np.trace(matrix) # trace of the matrix, the sum of the diagonal entries\n",
    "mix_matrix = np.sum(np.matmul(matrix, matrix))\n",
    "\n",
    "r1 = (trace-mix_matrix)/(1-mix_matrix) # Eq. 2\n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of 0.71 indicated some clusters of nodes are formed based on their top field, but there are also occurences where this is not the case. With 0 indicating random mixing we can see that their field plays some kind of role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the graph assortative with respect to the degree? (e.g. do high-degree scientists tend to link to other high-degree scientists, and low-degree scientists to other low-degree scientists?). Provide an interpretation of your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the assortativity coefficient is > 0, it suggests that nodes with similar degrees tend to be connected to each other, indicating assortative mixing w.r.t. the degree. And if the assortativity coefficient is < 0, it suggests that nodes with different degrees tend to be connected to each other, indicating disassortative mixing w.r.t. the degree. \n",
    "\n",
    "A value of 0.71 for the assortativity coefficient suggests that there is a degree of assortative mixing for the authors with respect to the their top fields. In other words, authors with the same top field are more likely to be connected to each other than to nodes with different top fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2: ZACHARYS'S KARATE CLUB**\n",
    "\n",
    "In this exercise, we will work on Zarachy's karate club graph (refer to the Introduction of Chapter 9). The dataset is available in NetworkX, by calling the function karate_club_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Visualize the graph using netwulf. Set the color of each node based on the club split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from netwulf import visualize\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "#First we set the color of each node based on the club split\n",
    "colors = ['red' if G.nodes[n]['club'] == 'Mr. Hi' else 'blue' for n in G.nodes()]\n",
    "\n",
    "#Then we add the colors to the node attributes\n",
    "for i, n in enumerate(G.nodes()):\n",
    "    G.nodes[n]['color'] = colors[i]\n",
    "\n",
    "#Visualize the graph\n",
    "visualize(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function to compute the modularity of a graph partitioning using equation 9.12 in the book: \n",
    "\n",
    "$M=\\sum_{c=1}^{n_c}\\left[\\frac{L_c}{L}-(\\frac{k_c}{2L})^2\\right]$ \n",
    "\n",
    "\n",
    "\n",
    "The function should take a networkX Graph and a partitioning as inputs and return the modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef compute_modularity(G,partitioning):\\n    L = G.number_of_edges()\\n    M = 0\\n    communities = set(partitioning.values())\\n\\n    for community in communities:\\n        nodes_in_community = [node for node, community_id in partitioning.items() if community_id == community]\\n        subgraph = G.subgraph(nodes_in_community)\\n        k_c = sum(dict(subgraph.degree()).values())\\n        L_c = subgraph.number_of_edges()\\n        M += L_c / L - (k_c / (2 * L)) ** 2\\n\\n    return M\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def compute_modularity(G,partitioning):\n",
    "    L = G.number_of_edges()\n",
    "    M = 0\n",
    "    communities = set(partitioning.values())\n",
    "\n",
    "    for community in communities:\n",
    "        nodes_in_community = [node for node, community_id in partitioning.items() if community_id == community]\n",
    "        subgraph = G.subgraph(nodes_in_community)\n",
    "        k_c = sum(dict(subgraph.degree()).values())\n",
    "        L_c = subgraph.number_of_edges()\n",
    "        M += L_c / L - (k_c / (2 * L)) ** 2\n",
    "\n",
    "    return M\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain in your own words the concept of modularity **REFRASE**\n",
    "\n",
    "Modularity is a measure of the structure of a network, which is a collection of nodes (also known as vertices) and the edges (also known as links) that connect them. The concept of modularity is based on the idea that a well-connected network can be divided into groups of nodes, known as communities or clusters, where the nodes within each group are more densely connected to each other than to nodes outside the group.\n",
    "\n",
    "Modularity is a measure of how well a network can be divided into such communities. Specifically, modularity quantifies the difference between the number of edges within communities and the expected number of edges within communities if the edges were placed randomly, while preserving the degree distribution of the nodes. In other words, modularity measures the extent to which the edges in a network are concentrated within communities, rather than being distributed randomly throughout the network.\n",
    "\n",
    "Modularity is typically expressed as a number between -1 and 1. A high positive value of modularity indicates that the network is highly modular, with many densely connected communities, while a low or negative value indicates that the network is less modular, with communities that are not well-defined. The concept of modularity has many applications in network science, including community detection, network visualization, and network comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the modularity of the Karate club split partitioning using the function you just wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modularity of the Karate club split partitioning using the function is 0.489\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def compute_modularity(G,partitioning):\n",
    "    L = G.number_of_edges()\n",
    "    M = 0\n",
    "    communities = set(partitioning.values())\n",
    "\n",
    "    for community in communities:\n",
    "        nodes_in_community = [node for node, community_id in partitioning.items() if community_id == community]\n",
    "        subgraph = G.subgraph(nodes_in_community)\n",
    "        k_c = sum(dict(subgraph.degree()).values())\n",
    "        L_c = subgraph.number_of_edges()\n",
    "        M += L_c / L - (k_c / (2 * L)) ** 2\n",
    "\n",
    "    return M\n",
    "\n",
    "#We start by loading the Karate club graph\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "#First we get the club split partitioning from the node attributes\n",
    "club_split = nx.get_node_attributes(G, \"club\")\n",
    "\n",
    "#Then we compute the modularity of the partitioning by using the function above\n",
    "modularity = compute_modularity(G, club_split)\n",
    "\n",
    "print(f\"The modularity of the Karate club split partitioning using the function is {modularity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We will now perform a small randomization experiment to assess if the modularity you just computed is statitically different from 0. To do so, we will implement the double edge swap algorithm. Given a network G, this algorithm creates a new network, such that each node has exactly the same degree as in the original network, but different connections. Here is how the algorithm works.\n",
    "\n",
    "    a. Create an identical copy of your original network.\n",
    "    \n",
    "    b. Consider two edges in your new network (u,v) and (x,y), such that u!=v and v!=x.\n",
    "    \n",
    "    c. If none of edges (u,y) and (x,v) exists already, add them to the network and remove edges (u,v) and (x,y).\n",
    "    \n",
    "Repeat steps b. and c. to achieve at least N swaps (I suggest N to be larger than the number of edges)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: \n",
    "Community detection on the network of Computational Social Scientists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Python Louvain-algorithm implementation to find communities. How many communities do you find? What are their sizes? Report the value of modularity found by the algorithm. Is the modularity significantly different than 0?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "import networkx as nx\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netwulf import visualize\n",
    "\n",
    "def read_json_file(filename):\n",
    "    with open(filename) as f:\n",
    "        js_graph = json.load(f)\n",
    "    return json_graph.node_link_graph(js_graph)\n",
    "G = read_json_file(\"data_total.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities: 254\n",
      "Community sizes: [3, 195, 87, 116, 5, 2, 2, 2, 259, 111, 2913, 2, 3, 2, 3, 2, 2, 2, 33, 12, 2024, 357, 2, 1130, 2, 5, 217, 210, 10, 14, 330, 2, 2, 2, 193, 4, 207, 3, 2, 237, 2, 2, 21, 2, 105, 3, 18, 2, 157, 137, 2, 2, 2, 5, 2, 477, 4, 776, 3, 4, 74, 3, 160, 3, 2, 2, 146, 2, 2, 2, 4, 3, 2, 2, 2, 2, 4, 2, 4, 4, 2, 3, 3, 4, 2, 27, 2, 27, 40, 3, 2, 2, 7, 4, 5, 3, 6, 2, 524, 228, 2, 2, 2, 3, 2, 2, 25, 3, 2, 254, 3, 2, 2, 6, 2, 2, 2, 2, 4, 3, 2, 3, 3, 3, 5, 3, 2, 160, 3, 2, 10, 2, 2, 2, 5, 3, 4, 2, 3, 13, 2, 7, 2, 2, 4, 2, 3, 2, 2, 2, 3, 121, 3, 12, 2, 8, 5, 4, 3, 30, 2, 3, 4, 2, 2, 2, 2, 7, 4, 6, 3, 3, 2, 3, 4, 2, 2, 2, 2, 2, 7, 3, 4, 4, 5, 2, 9, 3, 2, 10, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 29, 6, 2, 6, 9, 5, 5, 7, 12, 11, 2, 3, 5, 12, 2, 5, 3, 3, 6, 4, 2, 4, 7, 2, 2, 2, 3, 4, 9, 3, 11, 3, 2, 2, 6, 3, 8, 2, 2, 2, 2, 2, 6, 54, 18, 4, 2, 5, 4, 6, 8, 2]\n",
      "Modularity: 0.6632768922233399\n",
      "Modularity is not significantly different than 0\n"
     ]
    }
   ],
   "source": [
    "import community\n",
    "\n",
    "#Louvain algorithm\n",
    "partition = community.best_partition(G) # dictionary, keys are the nodes and values are communities for each node\n",
    "\n",
    "# modularity of partition\n",
    "modularity = community.modularity(partition, G) \n",
    "\n",
    "size = []\n",
    "for community_ in set(partition.values()):\n",
    "    temp = []\n",
    "    for part in partition:\n",
    "        if partition[part] == community_:\n",
    "            temp.append(part)\n",
    "    size.append(len(temp))\n",
    "\n",
    "print(\"Number of communities:\", len(set(partition.values())))\n",
    "print(\"Community sizes:\", size)\n",
    "print(\"Modularity:\", modularity)\n",
    "\n",
    "expected_modularity = community.modularity(partition, G, weight=None) # expected modularity of a null model with the same degree\n",
    "if modularity > expected_modularity:\n",
    "    print(\"modularity significantly different than 0\")\n",
    "else:\n",
    "    print(\"modularity not different than 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "\n",
    "G_cc = G.subgraph(largest_cc)\n",
    "\n",
    "visualize(G_cc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the assigned community to the author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'citation': 4,\n",
       " 'year': 1871.0,\n",
       " 'papers': 8,\n",
       " 'field': 'Art',\n",
       " 'name': 'Catherine Dâ€™ignazio',\n",
       " 'community': 20}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.set_node_attributes(G, partition, 'community')\n",
    "\n",
    "# Test\n",
    "G.nodes[1404354049]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4: TF-IDF and the Computational Social Science communities**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does TF stand for\n",
    "\n",
    "TF stands for term frequency. Term frequency is metric that tells us how many instances of a given word (term) there is in a document. The way we get the term frequency is by getting the set of words of a given document, and then we count how many times each word occurs and divide that by the total number of words. So if a word has a term frequency of 0.1, it means that of all the words in the document, this particular word occurs 10% of the time\n",
    "\n",
    "\n",
    "- What does IDF stand for\n",
    "\n",
    "IDF stand for inverse document frequency. IDF is a metric for how unique a word is to a given document. If the word occurs in every document we have, it is not really worth mentioning if we were asked to describe the given document\n",
    "\n",
    "\n",
    "Let us take an example of why TD-IDF is used to tell which words hold the most information about a given document\n",
    "\n",
    "If we were to only look at TF we would in most cases come to the conclusion that the word \"the\" is the most descriptive of a document, because it is by far the most used word in the english language. However because it is so popular it will most likely have an IDF of 0 in any corpus consisting of english documents, which means that the TF-IDF of \"the\" would in any english corpus yield 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # natural language processing toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data file\n",
    "path = r\"C:\\Users\\otto\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Kurser\\02467 Computational Social Science\\Code\\paper_abstract_df_tokens.parquet\"\n",
    "path2 = r\"C:\\Users\\otto\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Kurser\\02467 Computational Social Science\\Code\\author_ids_df_week6.parquet\"\n",
    "df_tokens = pd.read_parquet(path) # contains the abstracts of the papers, and the tokens (words) in the abstracts\n",
    "df_authors = pd.read_parquet(path2) # contains author ids, which community they belong to, and the authors degree (number of edges (people the author has co-authored with))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to find out which words are important for each community, so we're going to create several *large documents, one for each community*. Each document includes all the tokens of abstracts written by members of a given community.\n",
    "\n",
    "- Consider a community c\n",
    "- Find all the abstracts of papers written by a member of community c.\n",
    "- Create a long array that stores all the abstract tokens\n",
    "- Repeat for all the communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>community</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2402406</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2229725</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3119272</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author_id  community  degree\n",
       "9    2402406          2       2\n",
       "10   2229725          2       2\n",
       "11   3119272          2       2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first commuunity\n",
    "community_0 = df_authors[df_authors[\"community\"] == 2]\n",
    "\n",
    "community_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes a community as input and gives the paperids of the papers written by the authors in the community\n",
    "def get_paperids(community):\n",
    "    paperids = []\n",
    "    for author in community[\"author_id\"]:\n",
    "        ids = df_tokens[df_tokens[\"authorIds\"].apply(lambda x: author in x)][[\"paperId\"]]\n",
    "        if len(ids) > 0:\n",
    "            paperids.append(ids.values[0][0])\n",
    "    return list(set(paperids))\n",
    "\n",
    "\n",
    "# Get the tokens of a list of paperIds\n",
    "def get_tokens(paperids):\n",
    "    tokens = []\n",
    "    for paperid in paperids:\n",
    "        temp = df_tokens[df_tokens[\"paperId\"] == paperid][\"tokens\"].values\n",
    "        if len(temp) > 0:\n",
    "            # join the tokens into the same list\n",
    "            tokens.extend(temp[0])\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do this process for all communities\n",
    "\n",
    "# groupby community\n",
    "grouped = df_authors.groupby(\"community\")\n",
    "\n",
    "# for each community apply the two functions\n",
    "communities = grouped.apply(lambda x: get_tokens(get_paperids(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "community\n",
       "0      [give, thorough, analytic, characterization, l...\n",
       "1                                                     []\n",
       "2      [seed, noisy, information, members, realworld,...\n",
       "3      [point, instantiation, simpsons, paradox, covi...\n",
       "4      [paper, provides, overview, legal, framework, ...\n",
       "                             ...                        \n",
       "306    [present, reactive, beta, model, accounts, lev...\n",
       "307    [personalization, technologies, widely, used, ...\n",
       "308                                                   []\n",
       "309    [abstract, temporal, nature, humans, interacti...\n",
       "310                                                   []\n",
       "Length: 311, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now have a list of lists, where each list contains the tokens of the papers written by the authors in the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start by implementing the more general solution, and then we can ask for the top 5 communities etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 5 communites by number of authors\n",
    "top_5_index = df_authors.groupby(\"community\").count().sort_values(\"author_id\", ascending=False).head(5).index\n",
    "\n",
    "# get the tokens of the top 5 communities\n",
    "top_5_tokens = communities[top_5_index]\n",
    "\n",
    "# get the term frequency (TF) of the top 5 communities\n",
    "top_5_tf = [nltk.FreqDist(tokens) for tokens in top_5_tokens]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a corpus as an index so we can vary the size of the corpus\n",
    "corpus = communities[top_5_index]\n",
    "\n",
    "# get the term frequency (TF) of the corpus\n",
    "tf = [nltk.FreqDist(tokens) for tokens in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are going to anyway, let us just calculate for every word how many communities it appears in\n",
    "\n",
    "# let us start with getting the set of all words in the corpus\n",
    "all_words = set()\n",
    "for tokens in corpus:\n",
    "    all_words.update(tokens)\n",
    "\n",
    "\n",
    "# we can then use this to calculate the tf-idf for each word in each community\n",
    "tf_idf = [{word: tf[word] * np.log(len(corpus)/sum([word in tokens for tokens in corpus])) for word in all_words} for tf in top_5_tf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.484941211906902"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first entry\n",
    "temp = tf_idf[0]\n",
    "\n",
    "# order the words by tf-idf\n",
    "sorted(temp, key=temp.get, reverse=True)\n",
    "\n",
    "temp[\"populist\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
