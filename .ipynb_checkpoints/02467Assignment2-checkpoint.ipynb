{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02467 Assignment 2\n",
    "s204085 Cornelius Erichs & s204076 Natasha Hougaard & s194101 Otto Schmidt\n",
    "\n",
    "#### GitHub repository\n",
    "Here is our repository on GitHub: https://github.com/natasha0301/Assigment2-CSS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Mixing Patterns and Assortativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1: MIXING PATTERNS AND ASSTORTATIVITY**\n",
    "\n",
    "For each node, compute the fraction of edges that connect to a node that works in the same top field. Find the average value across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "import networkx as nx\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_json_file(filename):\n",
    "    with open(filename) as f:\n",
    "        js_graph = json.load(f)\n",
    "    return json_graph.node_link_graph(js_graph)\n",
    "G = read_json_file(\"data_total.json\")\n",
    "\n",
    "def same_field(G):\n",
    "    same_field_fractions = []\n",
    "    for node in G.nodes():\n",
    "        same_field_neighbors = 0\n",
    "        total_neighbors = 0\n",
    "        \n",
    "        for neighbor in G.neighbors(node):\n",
    "            if G.nodes[neighbor][\"field\"] == G.nodes[node][\"field\"]:\n",
    "                same_field_neighbors += 1\n",
    "            total_neighbors += 1\n",
    "        \n",
    "        if total_neighbors > 0:\n",
    "            same_field_fraction = same_field_neighbors / total_neighbors\n",
    "        else:\n",
    "            same_field_fraction = 0\n",
    "        same_field_fractions.append(same_field_fraction)\n",
    "    return same_field_fractions\n",
    "\n",
    "same_field_fractions = same_field(G)\n",
    "print(np.mean(same_field(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new graph, with the same nodes and edges, but where the association between nodes and field is shuffled. Compute the measure above for this randomized graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "shuffled_G = G.copy()\n",
    "\n",
    "fields = [G.nodes[node][\"field\"] for node in G.nodes()]\n",
    "random.shuffle(fields)\n",
    "\n",
    "for i, node in enumerate(shuffled_G.nodes()):\n",
    "    shuffled_G.nodes[node][\"field\"] = fields[i]\n",
    "    \n",
    "print(np.mean(same_field(shuffled_G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the point above 100 times (at least). Plot the distribution of the values obtained and compare it with the value you have found for the real graph. Is the chance to connect to a member of the same field significantly higher than it would be by chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "results = []\n",
    "for i in range(500):\n",
    "    random.shuffle(fields)\n",
    "\n",
    "    for i, node in enumerate(shuffled_G.nodes()):\n",
    "        shuffled_G.nodes[node][\"field\"] = fields[i]\n",
    "    \n",
    "    results.append(np.mean(same_field(shuffled_G)))\n",
    "    \n",
    "plt.hist(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the assortativity coefficient with respect to author's field. How do you interpret the value you obtain? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(fields))\n",
    "unique_fields = {'Agricultural And Food Sciences' : 0,\n",
    "            'Art' : 1,\n",
    "            'Business' : 2,\n",
    "            'Chemistry' : 3,\n",
    "            'Computer Science' : 4,\n",
    "            'Economics' : 5,\n",
    "            'Education' : 6,\n",
    "            'Engineering' : 7,\n",
    "            'Environmental Science' : 8,\n",
    "            'Geography' : 9,\n",
    "            'Geology' : 10,\n",
    "            'History' : 11,\n",
    "            'Law' : 12,\n",
    "            'Linguistics': 13,\n",
    "            'Materials Science': 14,\n",
    "            'Mathematics': 15,\n",
    "            'Medicine': 16,\n",
    "            'Philosophy' : 17,\n",
    "            'Physics' : 18,\n",
    "            'Political Science' : 19,\n",
    "            'Psychology' : 20,\n",
    "            'Sociology' : 21,\n",
    "            None:22}\n",
    "\n",
    "matrix = np.zeros((len(unique_fields),len(unique_fields)))\n",
    "\n",
    "values = nx.get_node_attributes(G, \"field\").values()\n",
    "num_values = len(values)\n",
    "\n",
    "for start, end in G.edges(): # Looping over all edges in graph (since its undirected its not really start and stop)\n",
    "    x = G.nodes[start][\"field\"] # Getting the start point of the edge \n",
    "    y = G.nodes[end][\"field\"] # Getting the end point of the edge \n",
    "    if x in unique_fields: \n",
    "        x = unique_fields[x]\n",
    "    else:\n",
    "        x = unique_fields[None] # in case x is nan\n",
    "        \n",
    "    if y in unique_fields:\n",
    "        y = unique_fields[y]\n",
    "    else:\n",
    "        y = unique_fields[None] # in case y is nan\n",
    "    matrix[x, y] += 1 \n",
    "    \n",
    "num_edges = len(G.edges())\n",
    "matrix /= num_edges # averaging the occurence with the total edges\n",
    "\n",
    "trace = np.trace(matrix) # trace of the matrix, the sum of the diagonal entries\n",
    "mix_matrix = np.sum(np.matmul(matrix, matrix))\n",
    "\n",
    "r1 = (trace-mix_matrix)/(1-mix_matrix) # Eq. 2\n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of 0.71 indicated some clusters of nodes are formed based on their top field, but there are also occurences where this is not the case. With 0 indicating random mixing we can see that their field plays some kind of role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the graph assortative with respect to the degree? (e.g. do high-degree scientists tend to link to other high-degree scientists, and low-degree scientists to other low-degree scientists?). Provide an interpretation of your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the assortativity coefficient is > 0, it suggests that nodes with similar degrees tend to be connected to each other, indicating assortative mixing w.r.t. the degree. And if the assortativity coefficient is < 0, it suggests that nodes with different degrees tend to be connected to each other, indicating disassortative mixing w.r.t. the degree. \n",
    "\n",
    "A value of 0.71 for the assortativity coefficient suggests that there is a degree of assortative mixing for the authors with respect to the their top fields. In other words, authors with the same top field are more likely to be connected to each other than to nodes with different top fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2: ZACHARYS'S KARATE CLUB**\n",
    "\n",
    "In this exercise, we will work on Zarachy's karate club graph (refer to the Introduction of Chapter 9). The dataset is available in NetworkX, by calling the function karate_club_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Visualize the graph using netwulf. Set the color of each node based on the club split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from netwulf import visualize\n",
    "\n",
    "G_karate = nx.karate_club_graph()\n",
    "\n",
    "#First we set the color of each node based on the club split\n",
    "colors = ['red' if G_karate.nodes[n]['club'] == 'Mr. Hi' else 'blue' for n in G_karate.nodes()]\n",
    "\n",
    "#Then we add the colors to the node attributes\n",
    "for i, n in enumerate(G_karate.nodes()):\n",
    "    G_karate.nodes[n]['color'] = colors[i]\n",
    "\n",
    "#Visualize the graph\n",
    "network, config = visualize(G_karate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function to compute the modularity of a graph partitioning using equation 9.12 in the book: \n",
    "\n",
    "$M=\\sum_{c=1}^{n_c}\\left[\\frac{L_c}{L}-(\\frac{k_c}{2L})^2\\right]$ \n",
    "\n",
    "\n",
    "\n",
    "The function should take a networkX Graph and a partitioning as inputs and return the modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def compute_modularity(G,partitioning):\n",
    "    L = G.number_of_edges()\n",
    "    M = 0\n",
    "    communities = set(partitioning.values())\n",
    "\n",
    "    for community in communities:\n",
    "        nodes_in_community = [node for node, community_id in partitioning.items() if community_id == community]\n",
    "        subgraph = G.subgraph(nodes_in_community)\n",
    "        k_c = sum(dict(subgraph.degree()).values())\n",
    "        L_c = subgraph.number_of_edges()\n",
    "        M += L_c / L - (k_c / (2 * L)) ** 2\n",
    "\n",
    "    return M\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain in your own words the concept of modularity\n",
    "\n",
    "Modularity measures the structure of a network, which contains nodes and the edges(links) that connect them. Modularity is based on the idea that a well-connected network can be divided into groups of nodes, known as communities or clusters, where the nodes within each group are more tightly connected to each other than to nodes outside the group. Modularity is a measure of how well a network can be divided into such communities. So, it is the difference between the number of edges within communities and the expected number of edges within communities if the edges were placed randomly, while preserving the degree distribution of the nodes. Modularity is typically expressed as a number between -1 and 1. A high positive value tells us that the network is very modular and has many densely connected communities, while a low or negative value indicates that the network is less modular, with communities that are not well-defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the modularity of the Karate club split partitioning using the function you just wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modularity of the Karate club split partitioning using the function is 0.489\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def compute_modularity(Graph,partitioning):\n",
    "    L = Graph.number_of_edges()\n",
    "    M = 0\n",
    "    communities = set(partitioning.values())\n",
    "\n",
    "    for community in communities:\n",
    "        nodes_in_community = [node for node, community_id in partitioning.items() if community_id == community]\n",
    "        subgraph = Graph.subgraph(nodes_in_community)\n",
    "        k_c = sum(dict(subgraph.degree()).values())\n",
    "        L_c = subgraph.number_of_edges()\n",
    "        M += L_c / L - (k_c / (2 * L)) ** 2\n",
    "\n",
    "    return M\n",
    "\n",
    "#We start by loading the Karate club graph\n",
    "G_karate = nx.karate_club_graph()\n",
    "\n",
    "#First we get the club split partitioning from the node attributes\n",
    "club_split = nx.get_node_attributes(G_karate, \"club\")\n",
    "\n",
    "#Then we compute the modularity of the partitioning by using the function above\n",
    "modularity = compute_modularity(G_karate, club_split)\n",
    "\n",
    "print(f\"The modularity of the Karate club split partitioning using the function is {modularity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We will now perform a small randomization experiment to assess if the modularity you just computed is statitically different from 0. To do so, we will implement the double edge swap algorithm. Given a network G, this algorithm creates a new network, such that each node has exactly the same degree as in the original network, but different connections. Here is how the algorithm works.\n",
    "\n",
    "    a. Create an identical copy of your original network.\n",
    "    \n",
    "    b. Consider two edges in your new network (u,v) and (x,y), such that u!=v and v!=x.\n",
    "    \n",
    "    c. If none of edges (u,y) and (x,v) exists already, add them to the network and remove edges (u,v) and (x,y).\n",
    "    \n",
    "Repeat steps b. and c. to achieve at least N swaps (I suggest N to be larger than the number of edges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def double_edge_swap(GG,N):\n",
    "    i = 0\n",
    "    G_copy = GG.copy()\n",
    "    while(i<2*N):\n",
    "        edges = list(G_copy.edges()) # update edges after adding and removal\n",
    "        (u, v), (x, y) = random.sample(edges, 2) # picking two random edges\n",
    "        if (u != v) and (v != x) and (u, y) not in G_copy.edges() and (x, v) not in G_copy.edges(): # checking conditions\n",
    "            G_copy.add_edge(u, y)\n",
    "            G_copy.add_edge(x, v)\n",
    "            G_copy.remove_edge(u, v)\n",
    "            G_copy.remove_edge(x, y)\n",
    "            i+=1\n",
    "    return G_copy\n",
    "G_new = double_edge_swap(G_karate,len(list(G_karate.edges())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Double check that your algorithm works well, by showing that the degree of nodes in the original network and the new 'randomized' version of the network are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 9, 10, 6, 3, 4, 4, 4, 5, 2, 3, 1, 2, 5, 2, 2, 2, 2, 2, 3, 2, 2, 2, 5, 3, 3, 2, 4, 3, 4, 4, 6, 12, 17]\n",
      "[16, 9, 10, 6, 3, 4, 4, 4, 5, 2, 3, 1, 2, 5, 2, 2, 2, 2, 2, 3, 2, 2, 2, 5, 3, 3, 2, 4, 3, 4, 4, 6, 12, 17]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "degree_1 = []\n",
    "for node in G_karate.nodes():\n",
    "    degree_1.append(G_karate.degree(node))\n",
    "degree_2 = []\n",
    "for node in G_new.nodes():\n",
    "    degree_2.append(G_new.degree(node))\n",
    "\n",
    "print(degree_1)\n",
    "print(degree_2)\n",
    "print(degree_1 == degree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Create 1000 randomized version of the Karate Club network using the double edge swap algorithm you wrote in step 5. For each of them, compute the modularity of the \"club\" split and store it in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(1000):\n",
    "    G_new = double_edge_swap(G_karate,len(list(G_karate.edges())))\n",
    "    club_split = nx.get_node_attributes(G_new, \"club\")\n",
    "    modularity = compute_modularity(G_new, club_split)\n",
    "    result.append(modularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Compute the average and standard deviation of the modularity for the random network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the modularity: 0.282\n",
      "Standard deviation of the modularity: 0.029\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(f\"Average of the modularity: {np.mean(result):.3f}\")\n",
    "print(f\"Standard deviation of the modularity: {np.std(result):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Plot the distribution of the \"random\" modularity. Plot the actual modularity of the club split as a vertical line (use axvline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual modularity: 0.489\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "club_split = nx.get_node_attributes(G_karate, \"club\")\n",
    "modularity = compute_modularity(G_karate, club_split)\n",
    "print(f\"Actual modularity: {modularity:.3f}\")\n",
    "\n",
    "plt.hist(result, label='Random')\n",
    "plt.axvline(modularity, color='green', label='Actual')\n",
    "plt.xlabel('Modularity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Distribution of Random Modularity and the line of the Actual Modularity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Comment on the figure. Is the club split a good partitioning? Why do you think I asked you to perform a randomization experiment? What is the reason why we preserved the nodes degree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the modularity of the \"random\" graphs center around 0.284, which is lower than the actual modularity. As modularity is a measure for the structure of a network, this indicates that the structure in the original graph is not entirely random.\n",
    "\n",
    "The reason we want to preserve the degree of each node is, that the degree plays a significant role when determining the community structure. If we did not preserve the nodes degree the distribtuion of the degrees would change, and so would the community structure aswell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Use the Python Louvain-algorithm implementation to find communities in this graph. Report the value of modularity found by the algorithm. Is it higher or lower than what you found above for the club split? What does this comparison reveal?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "#Louvain algorithm\n",
    "partition = community.best_partition(G_karate) # dictionary, keys are the nodes and values are communities for each node\n",
    "\n",
    "# modularity of partition\n",
    "modularity = community.modularity(partition, G_karate) \n",
    "\n",
    "print(f\"Modularity: {modularity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reason for the difference between Louvain and the modularity found before, could be because the partitioning of the graph is not clear cut, and that the communities can be partitioned in different ways. But they both indicate that there is some kind of structure, since their modularity is higher than that of the random graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Compare the communities found by the Louvain algorithm with the club split partitioning by creating a matrix D with dimension (2 times A), where A is the number of communities found by Louvain. We set entry D(i,j) to be the number of nodes that community i has in common with group split j. The matrix D is what we call a confusion matrix. Use the confusion matrix to explain how well the communities you've detected correspond to the club split partitioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start by loading the Karate club graph\n",
    "G_karate = nx.karate_club_graph()\n",
    "\n",
    "#First we get the club split partitioning from the node attributes\n",
    "club_split = nx.get_node_attributes(G_karate, \"club\")\n",
    "\n",
    "def convert_to_numb(club):\n",
    "    if club == \"Mr. Hi\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "A = max(partition.values())+1 # 4 partitions\n",
    "D = np.zeros((A,A))\n",
    "for i in range(A):\n",
    "    for j in range(A):\n",
    "        #print(j)\n",
    "        for node, club in club_split.items():\n",
    "            if partition[node] == i and convert_to_numb(club) == j:\n",
    "                D[i,j]+=1\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the club split only has two clubs, whereas Louvain has 4 different clubs. In the case where Louvain and club split was the same, we would have D[0,0] = 17 and D[1,1] = 17, and not 11 and 11. It did find a good partioning (when looking at the modularity), but as both seen in confusion matrix and the modularity, it did not find the \"correct\" partioning. When looking at the trace we see that is better than random, but just by some amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 3: COMMUNITY DETECTION ON THE NETWORK OF COMPUTATIONAL SOCIAL SCIENCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Python Louvain-algorithm implementation to find communities. How many communities do you find? What are their sizes? Report the value of modularity found by the algorithm. Is the modularity significantly different than 0?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "import networkx as nx\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netwulf import visualize\n",
    "\n",
    "def read_json_file(filename):\n",
    "    with open(filename) as f:\n",
    "        js_graph = json.load(f)\n",
    "    return json_graph.node_link_graph(js_graph)\n",
    "G = read_json_file(\"data_total.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "#Louvain algorithm\n",
    "partition = community.best_partition(G,random_state=42) # dictionary, keys are the nodes and values are communities for each node\n",
    "\n",
    "# modularity of partition\n",
    "modularity = community.modularity(partition, G) \n",
    "\n",
    "size = []\n",
    "for community_ in set(partition.values()):\n",
    "    temp = []\n",
    "    for part in partition:\n",
    "        if partition[part] == community_:\n",
    "            temp.append(part)\n",
    "    size.append(len(temp))\n",
    "\n",
    "print(\"Number of communities:\", len(set(partition.values())))\n",
    "print(\"Community sizes:\", sorted(size,reverse=True))\n",
    "print(f\"Modularity: {modularity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen the modularity is significantly larger than 0, indicated there is some community structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netwulf as nw\n",
    "import colorsys\n",
    "# Unable to visualise when using the graph with attributes, getting error \"file not found\"\n",
    "G2 = read_json_file(\"data_total_no_attr.json\") \n",
    "\n",
    "\n",
    "num_communities = len(set(partition.values()))\n",
    "hue_start = 0.0\n",
    "saturation = 0.8\n",
    "value = 0.8\n",
    "\n",
    "colors = []\n",
    "for i in range(num_communities):\n",
    "    hue = hue_start + (i / num_communities)\n",
    "    r, g, b = colorsys.hsv_to_rgb(hue, saturation, value)\n",
    "    color_hex = \"#{:02x}{:02x}{:02x}\".format(int(r * 255), int(g * 255), int(b * 255))\n",
    "    colors.append(color_hex)\n",
    "\n",
    "community_numb = list(partition.values())\n",
    "for i, n in enumerate(G.nodes()):\n",
    "    G2.nodes[n]['color'] = colors[community_numb[i]]\n",
    "\n",
    "#visualize(G2)\n",
    "\n",
    "network, config = visualize(G2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the assigned community to the author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, partition, 'community')\n",
    "\n",
    "# Test\n",
    "G.nodes[1404354049]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4: TF-IDF and the Computational Social Science communities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does TF stand for\n",
    "\n",
    "TF stands for term frequency. Term frequency is metric that tells us how many instances of a given word (term) there is in a document. The way we get the term frequency is by getting the set of words of a given document, and then we count how many times each word occurs and divide that by the total number of words. So if a word has a term frequency of 0.1, it means that of all the words in the document, this particular word occurs 10% of the time\n",
    "\n",
    "\n",
    "- What does IDF stand for\n",
    "\n",
    "IDF stand for inverse document frequency. IDF is a metric for how unique a word is to a given document. If the word occurs in every document we have, it is not really worth mentioning if we were asked to describe the given document\n",
    "\n",
    "\n",
    "Let us take an example of why TD-IDF is used to tell which words hold the most information about a given document\n",
    "\n",
    "If we were to only look at TF we would in most cases come to the conclusion that the word \"the\" is the most descriptive of a document, because it is by far the most used word in the english language. However because it is so popular it will most likely have an IDF of 0 in any corpus consisting of english documents, which means that the TF-IDF of \"the\" would in any english corpus yield 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # natural language processing toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data file\n",
    "df_tokens = pd.read_parquet(\"paper_abstract_df_tokens.parquet\") # contains the abstracts of the papers, and the tokens (words) in the abstracts\n",
    "df_authors = pd.read_parquet(\"author_ids_df_week6.parquet\") # contains author ids, which community they belong to, and the authors degree (number of edges (people the author has co-authored with))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to find out which words are important for each community, so we're going to create several *large documents, one for each community*. Each document includes all the tokens of abstracts written by members of a given community.\n",
    "\n",
    "- Consider a community c\n",
    "- Find all the abstracts of papers written by a member of community c.\n",
    "- Create a long array that stores all the abstract tokens\n",
    "- Repeat for all the communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes a community as input and gives the paperids of the papers written by the authors in the community\n",
    "def get_paperids(community):\n",
    "    paperids = []\n",
    "    for author in community[\"author_id\"]:\n",
    "        ids = df_tokens[df_tokens[\"authorIds\"].apply(lambda x: author in x)][[\"paperId\"]]\n",
    "        if len(ids) > 0:\n",
    "            paperids.append(ids.values[0][0])\n",
    "    return list(set(paperids))\n",
    "\n",
    "\n",
    "# Get the tokens of a list of paperIds\n",
    "def get_tokens(paperids):\n",
    "    tokens = []\n",
    "    for paperid in paperids:\n",
    "        temp = df_tokens[df_tokens[\"paperId\"] == paperid][\"tokens\"].values\n",
    "        if len(temp) > 0:\n",
    "            # join the tokens into the same list\n",
    "            tokens.extend(temp[0])\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do this process for all communities\n",
    "\n",
    "# groupby community\n",
    "grouped = df_authors.groupby(\"community\")\n",
    "\n",
    "# for each community apply the two functions\n",
    "communities = grouped.apply(lambda x: get_tokens(get_paperids(x)) if len(get_paperids(x)) > 0 else None)\n",
    "# we now have a list of lists, where each list contains the tokens of the papers written by the authors in the community\n",
    "# we drop the communities that have no papers\n",
    "communities.dropna(inplace=True)\n",
    "len(communities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start by implementing the more general solution, and then we can ask for the top 5 communities etc.\n",
    "# we define a corpus as an index so we can vary the size of the corpus\n",
    "corpus = communities\n",
    "\n",
    "# We define a function that given a corpus (list of lists) with tokens outputs all the words in the corpus\n",
    "def get_words(corpus):\n",
    "    words = set()\n",
    "    for tokens in corpus:\n",
    "        words.update(tokens)\n",
    "    return words\n",
    "\n",
    "# we define a function that calculates the tf / idf and outputs\n",
    "\n",
    "def tf_idf(corpus, df_authors, top_n:int=None):\n",
    "    data = dict()\n",
    "    if top_n:\n",
    "        top_n_index = df_authors.groupby(\"community\").count().sort_values(\"author_id\", ascending=False).head(top_n).index\n",
    "        corpus = communities[top_n_index]\n",
    "        data[\"top_n_index\"] = top_n_index\n",
    "    else:\n",
    "        corpus = communities\n",
    "    # calculate the term frequency (TF) of the corpus\n",
    "    tf_dist = [nltk.FreqDist(tokens) for tokens in corpus]\n",
    "\n",
    "    # Get all the words in the corpus\n",
    "    all_words = get_words(corpus)\n",
    "\n",
    "    # calculate tf-idf for the corpus\n",
    "    # only calculate tf-idf for the words in the TF\n",
    "    tf_idf = [{word: tf[word] * np.log(len(corpus)/sum([word in tokens for tokens in corpus])) for word in tf.keys()} for tf in tf_dist]\n",
    "    data[\"tf\"] = tf_dist\n",
    "    data[\"tf_idf\"] = tf_idf\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the tf-idf for the top 5 communities\n",
    "tf_idf_top_5 = tf_idf(corpus, df_authors, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe similarities and differences between the communities.\n",
    "- Why aren't the TFs not necessarily a good description of the communities?\n",
    "- Next, we calculate IDF for every word.\n",
    "- What base logarithm did you use? Is that important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at the TFs for the top 5 communities\n",
    "tf_idf_top_5[\"tf\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the term frequencies are quite similar for the top 5 communitiews. 4 out of 5 of them have social as their mosed used word, and 3 of them have data as their second.\n",
    "\n",
    "\n",
    "When we want to describe something the most important things to mention are what makes them special. so if we just used term frequency to describe a community, we would say that the word \"social\" describes them the best, however 4 of them have this as the most used word, so if we used this word to describe them, how would we be able to tell them apart. This is where tf-idf comes in\n",
    "\n",
    "For the IDF we use just **np.log** which is just the natural logarithm (base e). It does not matter which logarithm we use as all logarithms are asymptotically the same, and we can go from one base logarithm to another by the following formula:\n",
    "\n",
    "$$\n",
    "\\log _b a=\\frac{\\log _d a}{\\log _d b}\n",
    "$$\n",
    "\n",
    "So if wanted all the values of\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change of base from ln to log10\n",
    "np.log(10)/np.log(np.e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we wanted to change the base of the logarithm, we would just multiply all the values in the tf-idf by the given constant. And as we scale all the values by the same factor, it really doesnt matter which base we use.\n",
    "\n",
    "\n",
    "We have already calculated the TF-IDF for our corpus, we can access it in the directionary we returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_top_5[\"tf_idf\"]\n",
    "\n",
    "# however these are not sorted, so we need to sort them\n",
    "# we define a function that takes a list of dictionaries and sorts them by the values\n",
    "def sort_dict_list(dict_list):\n",
    "    return [dict(sorted(d.items(), key=lambda item: item[1], reverse=True)) for d in dict_list]\n",
    "\n",
    "# we now sort the tf-idf\n",
    "tf_idf_top_5[\"tf_idf\"] = sort_dict_list(tf_idf_top_5[\"tf_idf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us look at the top 10 words for each community\n",
    "for i, val in enumerate(tf_idf_top_5[\"tf_idf\"]):\n",
    "    print(f\"Community {i+1}\")\n",
    "    print(list(val.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the top 3 authors by degree\n",
    "df_authors.sort_values(\"degree\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see why tf-idf is useful, let us look at the word \"welfare\" in the 3rd entry which has a really high tf-idf\n",
    "# let us first see how many times the word \"welfare\" appears in the 3rd community\n",
    "tf_idf_top_5[\"tf\"][2][\"welfare\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears quite a lot in the 3rd community, Let us now see if it appears in the other communities\n",
    "[tf[\"welfare\"] for tf in tf_idf_top_5[\"tf\"]]\n",
    "\n",
    "# This gives us a good explaination of why the word \"welfare\" has a high tf-idf in the 3rd community. It appears a lot in the 3rd community, but not at all in the other communities. So it will have a very high tf-idf in the 3rd community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF for all communities\n",
    "tf_idf_top_9 = tf_idf(corpus, df_authors, top_n=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5: The Wordcloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF for top 9 communities\n",
    "tf_idf_top_9 = tf_idf(corpus, df_authors, top_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each community create a wordcloud\n",
    "for i in range(len(tf_idf_top_9[\"tf_idf\"])):\n",
    "    community_tf = tf_idf_top_9[\"tf\"][i]\n",
    "    community_tf_idf = tf_idf_top_9[\"tf_idf\"][i]\n",
    "    \n",
    "    if len(community_tf) < 1:\n",
    "        continue\n",
    "\n",
    "    # get the top 3 authors by degree in the community\n",
    "    # top_3_authors = df_authors[df_authors[\"community\"] == i].sort_values(\"degree\", ascending=False).head(3)[\"author_id\"].values\n",
    "    top_3_authors = df_authors[df_authors[\"community\"] == i].sort_values(\"degree\", ascending=False).head(3)[\"name\"].values\n",
    "    print(f\"Community {i} - Top 3 authors: {top_3_authors}\")\n",
    "    \n",
    "    wordcloud = WordCloud(width = 1000, height = 500, background_color=\"white\").generate_from_frequencies(community_tf_idf)\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Community {i}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
